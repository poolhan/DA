from sklearn.decomposition import NMF
from sklearn.feature_extraction.text import TfidfVectorizer
# ğŸ“˜ Gensim ê¸°ë°˜ LDA ëª¨ë¸ë§
import pandas as pd

# ë°ì´í„° ë¡œë“œ
df_raw = pd.read_parquet("mecab keyword.parquet")

from gensim.corpora import Dictionary
from gensim.models.ldamodel import LdaModel



# í‚¤ì›Œë“œë¥¼ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
df_raw["keywords"] = df_raw["keywords"].str.split()

#
# ğŸ” ìµœì ì˜ kê°’(í† í”½ ìˆ˜)ì„ ì°¾ìœ¼ë ¤ë©´ coherence score ë˜ëŠ” perplexity ê¸°ë°˜ ë°˜ë³µ ì‹¤í—˜ì´ í•„ìš”í•¨
# ì˜ˆ: ì—¬ëŸ¬ kê°’ì— ëŒ€í•´ coherence scoreë¥¼ ë¹„êµí•˜ê³  ê°€ì¥ ë†’ì€ k ì„ íƒ




# ì‚¬ìš©ì ì§€ì • ë¶ˆìš©ì–´ ìë™ ì ìš©
try:
    with open("custom_stopwords.txt", "r") as f:
        custom_stopwords = set(line.strip() for line in f if line.strip())
    df_raw["keywords"] = df_raw["keywords"].apply(lambda tokens: [t for t in tokens if t not in custom_stopwords])
    print(f"ğŸ§¹ ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ {len(custom_stopwords)}ê°œ ì ìš© ì™„ë£Œ")
except FileNotFoundError:
    print("âš ï¸ custom_stopwords.txt íŒŒì¼ì´ ì—†ì–´ ë¶ˆìš©ì–´ ì œê±°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

# Dictionary ë° Corpus ìƒì„±
dictionary = Dictionary(df_raw["keywords"].tolist())
# ë¬¸ì„œ ìˆ˜ ê¸°ì¤€ ë‹¨ì–´ í•„í„°ë§: ë„ˆë¬´ ë“œë¬¼ê±°ë‚˜ í”í•œ ë‹¨ì–´ ì œê±°
dictionary.filter_extremes(no_below=5, no_above=0.7)
corpus = [dictionary.doc2bow(tokens) for tokens in df_raw["keywords"]]

lda_gensim = LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=6,  # ì‚¬ìš©í•  í† í”½ ìˆ˜
    random_state=42,
    passes=20,
    iterations=400,
    alpha='auto',  # ë¬¸ì„œë³„ í† í”½ ë¶„í¬ ë™ì  ìµœì í™”
    eta=0.01       # í† í”½ë³„ ë‹¨ì–´ ì§‘ì¤‘ë„ ì¡°ì ˆ
)

# === ê° ë¬¸ì„œì˜ í† í”½ ë¶„í¬ ë²¡í„° ì¶”ì¶œ ===
topic_dist = [lda_gensim.get_document_topics(doc, minimum_probability=0.0) for doc in corpus]
topic_vectors = []
for dist in topic_dist:
    vec = [0.0] * lda_gensim.num_topics
    for topic_id, prob in dist:
        vec[topic_id] = prob
    topic_vectors.append(vec)

# === ë¹ˆë„ + ë¶„ì‚° ê¸°ë°˜ ë¶ˆìš©ì–´ í›„ë³´ ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ìš©) ===
import numpy as np
from collections import Counter

all_tokens = [token for tokens in df_raw["keywords"] for token in tokens]
token_counts = Counter(all_tokens)
token_freq_df = pd.DataFrame(token_counts.items(), columns=["token", "frequency"])

topic_columns = [f"topic_{i}" for i in range(lda_gensim.num_topics)]
topic_array = np.array(topic_vectors)
vocab = list(token_freq_df["token"])

token_topic_dist = {token: [0]*lda_gensim.num_topics for token in vocab}
for doc_idx, tokens in enumerate(df_raw["keywords"]):
    for token in tokens:
        if token in token_topic_dist:
            for t_idx, val in enumerate(topic_array[doc_idx]):
                token_topic_dist[token][t_idx] += val

topic_dist_df = pd.DataFrame.from_dict(token_topic_dist, orient="index", columns=topic_columns)
topic_dist_df = topic_dist_df.div(topic_dist_df.sum(axis=1), axis=0).fillna(0)
topic_dist_df["std_dev"] = topic_dist_df.std(axis=1)
topic_dist_df["token"] = topic_dist_df.index

token_stats_df = pd.merge(token_freq_df, topic_dist_df[["token", "std_dev"]], on="token")
freq_thresh = token_stats_df["frequency"].quantile(0.9)
std_thresh = token_stats_df["std_dev"].quantile(0.4)

token_stats_df["type"] = "ë³´ë¥˜"
token_stats_df.loc[(token_stats_df["frequency"] >= freq_thresh) & (token_stats_df["std_dev"] <= std_thresh), "type"] = "ì£¼ì œì–´ í›„ë³´"
token_stats_df.loc[(token_stats_df["frequency"] >= freq_thresh) & (token_stats_df["std_dev"] > std_thresh), "type"] = "ë¶ˆìš©ì–´ í›„ë³´"

print("\nğŸ“‹ ë¶„ì‚°+ë¹ˆë„ ê¸°ë°˜ ë¶ˆìš©ì–´ í›„ë³´:")
for word in token_stats_df[token_stats_df["type"] == "ë¶ˆìš©ì–´ í›„ë³´"]["token"].tolist():
    print(f"- {word}")




# í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ì¶œë ¥
print("\nğŸ“Œ Gensim LDA ì£¼ìš” í† í”½:")
for i, topic in lda_gensim.print_topics(num_words=10):
    print(f"Topic {i}: {topic}")

# --- Save topâ€‘20 keywords per topic to CSV ---
import csv
csv_path = "topic_top20_keywords.csv"
with open(csv_path, "w", encoding="utf-8", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(["topic_id", "top20_keywords"])
    for t in range(lda_gensim.num_topics):
        words = [w for w, _ in lda_gensim.show_topic(t, topn=100)]
        writer.writerow([t, " ".join(words)])
print(f"ğŸ“ í† í”½ë³„ ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ ì €ì¥ ì™„ë£Œ: {csv_path}")

# ê° í† í”½ë³„ ëŒ€í‘œ ë¬¸ì„œ 3ê°œì”© ì¶œë ¥
print("\nğŸ“Œ í† í”½ë³„ ëŒ€í‘œ ë¬¸ì„œ:")
import numpy as np

topic_dist = [lda_gensim.get_document_topics(doc, minimum_probability=0.0) for doc in corpus]
topic_vectors = []
for dist in topic_dist:
    vec = [0.0] * lda_gensim.num_topics
    for topic_id, prob in dist:
        vec[topic_id] = prob
    topic_vectors.append(vec)

topic_doc_map = {i: [] for i in range(lda_gensim.num_topics)}

# ğŸ“Š ê° í† í”½ì— ì†í•œ ë¬¸ì„œ ìˆ˜ í™•ì¸
topic_doc_count = [0] * lda_gensim.num_topics
for vec in topic_vectors:
    dominant_topic = np.argmax(vec)
    topic_doc_count[dominant_topic] += 1
print("\nğŸ“Š ê° í† í”½ì— ì†í•œ ë¬¸ì„œ ìˆ˜:")
for topic_id, count in enumerate(topic_doc_count):
    print(f"Topic {topic_id}: {count}ê°œ ë¬¸ì„œ")

for doc_idx, doc in enumerate(topic_vectors):
    dominant_topic = np.argmax(doc)
    topic_doc_map[dominant_topic].append((doc_idx, doc[dominant_topic]))


# ğŸ“ í† í”½ë³„ ëŒ€í‘œ ë¬¸ì„œ ì €ì¥
with open("topic_representative_docs.txt", "w", encoding="utf-8") as f:
    for topic_id, doc_scores in topic_doc_map.items():
        f.write(f"\nğŸ”· Topic {topic_id + 1} ëŒ€í‘œ ë¬¸ì„œ:\n")
        import random
        top_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)[:15]
        for doc_idx, score in top_docs:
            isbn = df_raw.iloc[doc_idx]['isbn']
            f.write(f"- ISBN {isbn} (score: {score:.4f})\n")
            f.write(f"  â†’ í† í°: {' '.join(df_raw.iloc[doc_idx]['keywords'])}\n")
print("ğŸ“ í† í”½ë³„ ëŒ€í‘œ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ: topic_representative_docs.txt")

# ğŸ“ í† í”½ë³„ ëŒ€í‘œ ISBNë§Œ ì €ì¥
with open("topic_representative_isbns.txt", "w", encoding="utf-8") as f:
    for topic_id, doc_scores in topic_doc_map.items():
        f.write(f"Topic {topic_id + 1}:\n")
        top_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)[:15]
        for doc_idx, _ in top_docs:
            isbn = df_raw.iloc[doc_idx]['isbn']
            f.write(f"{isbn}\n")
print("ğŸ“ í† í”½ë³„ ëŒ€í‘œ ISBN ì €ì¥ ì™„ë£Œ: topic_representative_isbns.txt")


# ğŸ“Š pyLDAvis ê¸°ë°˜ LDA ì‹œê°í™” ë° HTML ì €ì¥
if __name__ == "__main__":
    import pyLDAvis.gensim_models
    import pyLDAvis

    print("ğŸ“Š pyLDAvis ì‹œê°í™” HTML ìƒì„± ì¤‘...")
    vis = pyLDAvis.gensim_models.prepare(lda_gensim, corpus, dictionary)

    # í† í”½ ë²ˆí˜¸ë¥¼ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ pyLDAvis ë‚´ë¶€ ë°ì´í„° ìˆ˜ì •
    vis_data = vis.to_dict()
    vis_data["mdsDat"]["topics"] = [i - 1 for i in vis_data["mdsDat"]["topics"]]
    vis_data["mdsDat"]["names"] = [str(i) for i in range(len(vis_data["mdsDat"]["topics"]))]
    vis = pyLDAvis.PreparedData(
        mds_df=vis_data["mdsDat"],
        tinfo=vis_data["tinfo"],
        token_table=vis_data["token.table"],
        R=vis_data["R"]
    )
    pyLDAvis.save_html(vis, 'lda_gensim_vis.html')
    print("âœ… ì‹œê°í™” ì €ì¥ ì™„ë£Œ: lda_gensim_vis.html (ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ë³´ì„¸ìš”)")



# ğŸ“ Gensim LDA ë²¡í„° ì €ì¥
df_vectors = pd.DataFrame(topic_vectors, columns=[f"topic_{i}" for i in range(lda_gensim.num_topics)])
df_vectors["isbn"] = df_raw["isbn"].values
df_vectors.to_parquet("book_gensim_lda_vectors.parquet", index=False)
print("âœ… Gensim LDA ë²¡í„° ì €ì¥ ì™„ë£Œ: book_gensim_lda_vectors.parquet")

# ğŸ“ ë¶ˆìš©ì–´ í›„ë³´ ì €ì¥
stopword_candidates = token_stats_df[token_stats_df["type"] == "ë¶ˆìš©ì–´ í›„ë³´"]["token"].tolist()
with open("stopword_candidates.txt", "w", encoding="utf-8") as f:
    for word in stopword_candidates:
        f.write(f"{word}\n")
print("ğŸ“ ë¶ˆìš©ì–´ í›„ë³´ ì €ì¥ ì™„ë£Œ: stopword_candidates.txt")

# ğŸ”’ í•™ìŠµëœ ë² ìŠ¤íŠ¸ì…€ëŸ¬ LDA ëª¨ë¸ê³¼ ì‚¬ì „ ì €ì¥
lda_gensim.save("bestseller_lda.model")
dictionary.save("bestseller_dictionary.dict")
print("âœ… LDA ëª¨ë¸ê³¼ Dictionary ì €ì¥ ì™„ë£Œ: bestseller_lda.model, bestseller_dictionary.dict")


