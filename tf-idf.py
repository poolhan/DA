import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA

# íŒŒì¼ ë¡œë“œ
df_raw = pd.read_parquet("df_raw_with_keywords.parquet")

# ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ í•„í„°ë§
try:
    with open("custom_stopwords.txt", "r") as f:
        stopwords = set(line.strip() for line in f if line.strip())
    df_raw["keywords"] = df_raw["keywords"].apply(
        lambda tokens: [t for t in tokens.split() if t not in stopwords]
    )
    df_raw["keywords"] = df_raw["keywords"].apply(lambda tokens: " ".join(tokens))
    print(f"ğŸ§¹ ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ {len(stopwords)}ê°œ ì ìš© ì™„ë£Œ")
except FileNotFoundError:
    print("âš ï¸ custom_stopwords.txt íŒŒì¼ì´ ì—†ì–´ ë¶ˆìš©ì–´ ì œê±°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

# TF-IDF ë²¡í„°í™”
print("ğŸ“Œ í–¥ìƒëœ TF-IDF ì ìˆ˜ ê³„ì‚° ì¤‘...")
tfidf = TfidfVectorizer(min_df=10, max_df=0.7)
X = tfidf.fit_transform(df_raw["keywords"])
print("TF-IDF í–‰ë ¬:", X.shape)

# TF-IDF í–‰ë ¬ ì €ì¥
feature_names = tfidf.get_feature_names_out()
df_tfidf = pd.DataFrame(X.toarray(), columns=feature_names)
df_tfidf["isbn"] = df_raw["isbn"].values
df_tfidf.to_parquet("book_tfidf_vectors.parquet", index=False)
print("âœ… ì €ì¥ ì™„ë£Œ: book_tfidf_vectors.parquet")

# PCA ì ìš©
pca = PCA(n_components=0.95, random_state=42)
X_pca = pca.fit_transform(X.toarray())
print("ì„ íƒëœ ì£¼ì„±ë¶„ ìˆ˜:", pca.n_components_)
print("ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨:", pca.explained_variance_ratio_)

# DataFrame ë³€í™˜ ë° ISBN í¬í•¨
df_pca = pd.DataFrame(X_pca, columns=[f"PC{i+1}" for i in range(pca.n_components_)])
df_pca["isbn"] = df_raw["isbn"].values

# ì €ì¥
df_pca.to_parquet("book_keywords_pca.parquet", index=False)
print("âœ… ì €ì¥ ì™„ë£Œ: book_keywords_pca.parquet")